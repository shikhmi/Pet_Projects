# -*- coding: utf-8 -*-
"""–ö–æ–ø–∏—è –±–ª–æ–∫–Ω–æ—Ç–∞ "MSU_hw1_bpe_and_transformer.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/119wjROdAqq4iWK1LdrWPIqGU0P3rA8yU

# –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ç-–ø—Ä–æ–µ–∫—Ç–∞ (–ø—Ä–∏–¥—É–º–∞—Ç—å)

–í —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ —è –∑–∞–Ω–∏–º–∞–ª–∞—Å—å –æ–±—É—á–µ–Ω–∏–µ–º Byte-level BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –Ω–µ–±–æ–ª—å—à–æ–π LM.  

–ü—Ä–æ–µ–∫—Ç —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤:
1. —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞,
2. —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformer –º–æ–¥–µ–ª–∏,
3. –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å —Ä—É—Å—Å–∫–∏–º–∏ –∞–Ω–µ–∫–¥–æ—Ç–∞–º–∏
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --quiet datasets livelossplot

import inspect
import json
import os
import math
from collections import Counter
from dataclasses import dataclass
from functools import lru_cache, partial
from pathlib import Path

import regex as re
import torch
import torch.nn as nn
from datasets import load_dataset
from huggingface_hub import HfApi, PyTorchModelHubMixin, interpreter_login, snapshot_download
from huggingface_hub.utils import SoftTemporaryDirectory
from livelossplot import PlotLosses
from torch import Tensor
from torch.nn import functional as F
from torch.utils.data import DataLoader
from tqdm.auto import tqdm, trange

interpreter_login()

username = HfApi().whoami()["name"]
REPO_NAME = f"{username}/nlp_hehehe"  # –ò–ª–∏ –∫–∞–∫ –≤–∞–º —Ö–æ—á–µ—Ç—Å—è

print(f"Homework repository: '{REPO_NAME}'")

# –ò –¥—Ä—É–≥–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ –≤–µ—â–∏
SEED = 0xC0FFEE

"""# –î–∞—Ç–∞—Å–µ—Ç

–ü–µ—Ä–≤—ã–º –¥–µ–ª–æ–º –∑–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ: [ü§ó IgorVolochay/russian_jokes](https://huggingface.co/datasets/IgorVolochay/russian_jokes)

–ò –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –Ω–∏—Ö üëÄ
"""

dataset = load_dataset("json", data_files="hf://datasets/IgorVolochay/russian_jokes/dataset.json")
print("\n===\n".join(dataset["train"]["jokes"][:3]))

# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ö–æ–ª–¥–∞—É—Ç—ã
dataset = dataset["train"].train_test_split(test_size=0.1, seed=SEED)
dataset

"""# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä

–í –∫–∞—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç Byte-level BPE.

–î–ª—è —ç—Ç–æ–≥–æ:
1. –†–µ–∞–ª–∏–∑—É–µ–º –µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –Ω–∞–±–æ—Ä —Å–ª–∏—è–Ω–∏–π –ø–æ —ç—Ç–æ–º—É —Å–ª–æ–≤–∞—Ä—é
2. –û–±—É—á–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ
3. –†–µ–∞–ª–∏–∑—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤

"""

# regular expression for tokenization
WHITESPACE_SPLITTER = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

def bytes_to_unicode() -> dict[int, str]:
    """The original dictionary consists of 256 bytes and their corresponding Unicode characters.
    For example, chr(33) is '!'. However, not all bytes have a visually appealing representation,
    so such characters are skipped and replaced with the first available ones, i.e. shifted by 256.
    """
    initial_bytes = (
        list(range(ord("!"), ord("~") + 1)) + list(range(ord("¬°"), ord("¬¨") + 1)) + list(range(ord("¬Æ"), ord("√ø") + 1))
    )
    initial_chars = [chr(it) for it in initial_bytes]
    n = 0
    for byte in range(2**8):
        if byte not in initial_bytes:
            initial_bytes.append(byte)
            initial_chars.append(chr(2**8 + n))
            n += 1
    return dict(sorted(zip(initial_bytes, initial_chars)))

def merge(merge_pair: tuple[str, str],
          pair_frequences: Counter[tuple[str, str]],
          words_by_tokens: Counter[tuple[str]]):
    """Merges a given pair of tokens and update corresponding stats

    Args:
        merge_pair: The pair of tokens to be merged.
        pair_frequences: A counter tracking the frequency of token pairs in the dataset.
        words_by_tokens: A counter mapping tokenized words to their frequencies.

    Returns:
        Updated pair frequences and word tokenization w.r.t. to new token.
    """
    a, b = merge_pair
    new_token = a + b

    new_token_count = Counter()

    for word, freq in words_by_tokens.items():
      i = 0
      merge = []
      while i < len(word):
        if i < len(word) -1 and word[i] == a and word[i + 1] == b:
          merge.append(new_token)
          i +=2
        else:
          merge.append(word[i])
          i+=1
      new_token_count[tuple(merge)] +=freq

    new_pair_freq = Counter()

    for word, freq in new_token_count.items():
      for i in range(len(word) - 1):
        pair = [word[i], word[i+1]]
        new_pair_freq[tuple(pair)] += freq
    return new_token_count, new_pair_freq

def merge(merge_pair: tuple[str, str],
          pair_frequences: Counter[tuple[str, str]],
          words_by_tokens: Counter[tuple[str]]):

    """Merges a given pair of tokens and update corresponding stats

    Args:
        merge_pair: The pair of tokens to be merged.
        pair_frequences: A counter tracking the frequency of token pairs in the dataset.
        words_by_tokens: A counter mapping tokenized words to their frequencies.

    Returns:
        Updated pair frequences and word tokenization w.r.t. to new token.
    """

    a, b = merge_pair
    new_token = a + b

    delta_words = Counter()   # word -> +/- freq

    for word, freq in list(words_by_tokens.items()):
        has_pair = False
        for i in range(len(word)-1):
          if word[i] == a and word[i+1] == b:
            has_pair = True
            break
        if not has_pair:
            continue

        for i in range((len(word) - 1)):
          pair_frequences[(word[i], word[i+1])] -= freq

        new_word_list = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and word[i] == a and word[i+1] == b:
                new_word_list.append(new_token)
                i += 2
            else:
                new_word_list.append(word[i])
                i += 1
        new_word = tuple(new_word_list)

        for i in range((len(new_word) - 1)):
          pair_frequences[(new_word[i], new_word[i+1])] += freq

        delta_words[word] -= freq
        delta_words[new_word] += freq

    words_by_tokens.update(delta_words)

    for i in (list(words_by_tokens.keys())):
       if words_by_tokens[i] <= 0:
        del words_by_tokens[i]

    for i in (list(pair_frequences.keys())):
       if pair_frequences[i] <= 0:
        del pair_frequences[i]

    return pair_frequences, words_by_tokens

def train(data: list[str], vocab_size: int = 1024, special_tokens: list[str] = None):
    """Train BPE tokenizer on passed data

    Args:
        data: List of train documents
        vocab_size: Size of target vocabulary
        special_tokens: List of special tokens to add into vocabulary
    Returns:
        vocabulary: mapping from string token to id
        merges: list of merges, each one is tuple of string tokens
    """
    if vocab_size < 256:
        raise ValueError("Vocab size can't be less than 256")
    if special_tokens is None:
        special_tokens = []

    id2token = bytes_to_unicode()
    merges = []

    words_by_tokens = Counter()
    for sample in tqdm(data, desc="Loading data"):
        words = WHITESPACE_SPLITTER.findall(sample.strip())
        for word in words:
            word_byte = word.encode('utf-8')
            if not word_byte:
              continue
            tokens = tuple(id2token[b] for b in word_byte)
            words_by_tokens[tokens] += 1

    pair_frequences = Counter()

    for word, freq in list(words_by_tokens.items()):
        for i in range(len(word)-1):
          pair_frequences[(word[i], word[i+1])] += freq


    pbar = trange(vocab_size, desc="Building vocabulary", initial=len(id2token) + len(special_tokens))
    while len(id2token) < vocab_size - len(special_tokens):
        if len(pair_frequences) == 0:
            print("Not enough data to fulfil vocabulary")
            break

        top_pair = max(pair_frequences, key=pair_frequences.get)
        new_token = top_pair[0] + top_pair[1]
        del pair_frequences[top_pair]

        if new_token in id2token.values():
            continue
        id2token[len(id2token)] = new_token
        merges.append(top_pair)

        pair_frequences, words_by_tokens = merge(top_pair, pair_frequences, words_by_tokens)

        pbar.update()
    pbar.close()

    for special_token in special_tokens:
        id2token[len(id2token)] = special_token

    return {v: k for k, v in id2token.items()}, merges

# –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö

vocab, merges = train(dataset["train"]["jokes"], vocab_size=1024, special_tokens=["[EOS]"])

# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã

random_tokens = [512, 614, 768, 888, 1022]
unicode_to_bytes = {v: k for k, v in bytes_to_unicode().items()}
for token_id in random_tokens:
    token = [k for k, v in vocab.items() if v == token_id][0]
    raw_bytes = bytes([unicode_to_bytes[it] for it in token])
    print(f"Token #{token_id}: '{raw_bytes.decode('utf-8', errors='replace')}'")

class ByteLevelBPETokenizer:

    def __init__(self, vocab: dict[str, int], merges: list[tuple[str, str]], eos_token: str = "[EOS]"):
        """Byte-Level BPE Tokenizer

        Args:
            vocab: mapping from string token to id
            merges: list of merges in prioritized order
            eos_token: string representation of EOS token
        """
        super().__init__()
        if eos_token not in vocab:
            raise ValueError("There is no EOS token in vocab")
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        self.token2id = vocab
        self.id2token = {v: k for k, v in self.token2id.items()}
        self.eos_token = eos_token
        self.eos_token_id = self.token2id[eos_token]

        # The closer the pair is to the beginning, the higher the rank
        self.merges = merges
        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}

    @lru_cache
    def bpe(self, word: tuple[str]) -> tuple[str]:
        """Process word into tokenized representation.
        Word is a tuple of base tokens, i.e. bytes.

        Under the hood:
        1. Tracks the set of token pairs, bi-grams
        2. While possible, replaces the highest-ranking pair with its union

        Args:
            word: list of base string tokens
        Return:
            list of BPE tokens
        """
        if len(word) < 2:
          return word
        pairs = {(word[i], word[i+1]) for i in range (len(word) -1)}

        while True:
          best_rank = None
          best_pair = None
          for pair in pairs:
            rank = self.bpe_ranks.get(pair)
            if rank is not None and (best_rank is None or rank < best_rank):
              best_pair = pair
              best_rank = rank

          if best_pair is None:
            break

          fir, sec = best_pair
          new_word_list = []
          i = 0
          while i < len(word):
            if i < len(word) - 1 and word[i] == fir and word[i+1] == sec:
              new_word_list.append(fir + sec)
              i +=2
            else:
              new_word_list.append(word[i])
              i +=1

          word = tuple(new_word_list)
          if len(word) < 2:
            break
          else:
            pairs = {(word[i], word[i+1]) for i in range (len(word) -1)}

        return word


    def encode(self, text: str, add_eos_token: bool = True) -> list[int]:
        """Convert string to list of token ids.

        Args:
            text: input string, may contain multiple words
            add_eos_token: whether to add eos token id at the end
        Return:
            list of ints, ids of tokenized text
        """
        words = WHITESPACE_SPLITTER.findall(text)
        ids = []
        for w in words:
          enc_w = w.encode('utf-8')
          if not enc_w:
            continue

          base_token = tuple(self.byte_encoder[b] for b in enc_w)
          bpe_token = self.bpe(base_token)
          for t in bpe_token:
            tok_id = self.token2id.get(t)
            if tok_id is None:
              for ch in t:
                ids.append(self.token2id[self.byte_encoder[self.byte_decoder[ch]]])
            else:
              ids.append(tok_id)
        if add_eos_token:
          ids.append(self.eos_token_id)
        return ids


    def decode(self, idx: list[int]) -> str:
        """Convert list of tokens' ids to text, opposite to encode method

        Args:
            idx: list of tokens' ids
        Return:
            string, decoded text
        """
        if idx and idx[-1] == self.eos_token_id:
          ids = idx[:-1]
        else:
          ids = idx

        dec_str = ''
        dec_str_l = []
        for id in ids:
          str_token = self.id2token[id]
          dec_str_l.append(str_token)
        dec_str = ''.join(dec_str_l)

        text_str = bytes(self.byte_decoder[i] for i in dec_str)
        dec_w = text_str.decode('utf-8', errors='replace')

        return dec_w


    def push_to_hub(self, repo_id, *, private=None, token=None):
        api = HfApi()
        repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id

        # Push the files to the repo in a single commit
        with SoftTemporaryDirectory() as tmp:
            save_directory = Path(tmp) / repo_id
            save_directory.mkdir(parents=True)
            with open(save_directory / "vocabulary.json", "w") as f_out:
                print(json.dumps(self.token2id, indent=2), file=f_out)
            with open(save_directory / "merges.json", "w") as f_out:
                print(json.dumps({"merges": self.merges}), file=f_out)

            return api.upload_folder(repo_id=repo_id, folder_path=save_directory, token=token)

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *, token=None, **model_kwargs):
        if not os.path.isdir(pretrained_model_name_or_path):
            storage_folder = snapshot_download(repo_id=pretrained_model_name_or_path, token=token)
        else:
            storage_folder = pretrained_model_name_or_path
        storage_folder = Path(storage_folder)
        with open(storage_folder / "vocabulary.json", "r") as f_in:
            vocab = json.load(f_in)
        with open(storage_folder / "merges.json", "r") as f_in:
            merges = [tuple(it) for it in json.load(f_in)["merges"]]
        return cls(vocab, merges, **model_kwargs)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä


tokenizer = ByteLevelBPETokenizer(vocab, merges)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ö–∞–±

tokenizer.push_to_hub(REPO_NAME)

# –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ö–∞–±–∞

tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)

# –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞–±–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

text = "–ß—Ç–æ –±—ã–ª–æ –ø–æ–ª–≥–æ–¥–∞ –Ω–∞–∑–∞–¥? –ü–æ–º–∏–º–æ –≥—Ä–∞–Ω–¥–∏–æ–∑–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π, –ø–æ–ª–≥–æ–¥–∞ –Ω–∞–∑–∞–¥ –±—ã–ª–∏ –µ—â—ë —Å–µ–º–∏–Ω–∞—Ä—ã –ø–æ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä–µ."
ids = tokenizer.encode(text)
print(ids)
reverse_text = [tokenizer.decode([it]) for it in ids]
print("|".join(reverse_text))
print(tokenizer.decode(ids))

# –ü–æ—Å—á–∏—Ç–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–∏–º—Å—è —Å —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É –º–æ–¥–µ–ª–∏

lens = []
for text in tqdm(dataset["test"]["jokes"]):
    ids = tokenizer.encode(text)
    lens.append(len(ids))

print(f"Average token len per sample: {sum(lens) / len(lens):.2f}")
print(f"Minimum and maximum lens are: {min(lens)} and {max(lens)}")

"""# –ú–æ–¥–µ–ª—å

–í –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ —Ä–µ–∞–ª–∏–∑—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º
1. –í –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ALiBi
2. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GQA
3. –í Feed-Forward –±–ª–æ–∫–µ SwiGLU
"""

# –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –º–æ–¥–µ–ª–∏
@dataclass
class TransformerConfig:
    n_layer: int
    n_head: int
    n_kv_head: int
    hidden_dim: int
    intermediate_dim: int
    dropout: float = 0.1
    vocab_size: int = 1024
    max_seq_len: int = 128


model_configs = {
    "nano": TransformerConfig(n_layer=3, n_head=4, n_kv_head=2, hidden_dim=96, intermediate_dim=256),
    "mini": TransformerConfig(n_layer=6, n_head=6, n_kv_head=3, hidden_dim=384, intermediate_dim=1024),
    "small": TransformerConfig(n_layer=12, n_head=12, n_kv_head=6, hidden_dim=768, intermediate_dim=2048),
}

class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        """Root Mean Square Layer Normalization

        Args:
            dim: Feature dimension
            eps: Small constant for numerical stability
        """
        super().__init__()
        self.eps = eps
        self.scale = nn.Parameter(torch.ones(dim))

    def forward(self, x: Tensor) -> Tensor:
        rms = (self.eps + torch.mean(x**2, dim=-1, keepdim=True))**0.5
        norm_x = x/rms * self.scale

        return norm_x

class CausalSelfAttention(nn.Module):
    def __init__(self, config: TransformerConfig):
        """Causal Self-Attention with support of
        Grouped-Query Attention and ALiBi for positional encoding
        """
        super().__init__()
        self.config = config
        assert self.config.hidden_dim % self.config.n_head == 0
        assert self.config.n_head % self.config.n_kv_head == 0
        self.head_dim = self.config.hidden_dim // self.config.n_head
        self.scale = self.head_dim**-0.5
        self.q_per_kv = self.config.n_head // self.config.n_kv_head

        # Init projection layers
        self.q_proj = nn.Linear(self.config.hidden_dim, self.config.n_head * self.head_dim, bias = False)
        self.kv_proj = nn.Linear(self.config.hidden_dim, 2 * self.config.n_kv_head * self.head_dim, bias = False)
        self.out_proj = nn.Linear(self.config.hidden_dim, self.config.hidden_dim, bias = False)

        self.attn_dropout = nn.Dropout(self.config.dropout)

        self.register_buffer("causal_mask", self._create_causal_mask(self.config.max_seq_len))
        self.register_buffer("alibi", self._build_alibi_bias(self.config.n_head))

    def _build_alibi_bias(self, num_heads: int) -> Tensor:
        """Build ALiBi for specified number of heads:

        Returns:
            Tensor with ALiBi biases, shape: [1, num heads, 1, 1]
        """
        n = 2 ** math.floor(math.log2(num_heads))
        m_0 = 2.0 ** (-8.0 / n)
        m = torch.pow(m_0, torch.arange(1, 1 + n))
        if n < num_heads:
          m_0_hat = 2.0 ** (-4.0 / n) #—Ç—É—Ç –º–± –∫–æ—Å—è–∫
          m_hat = torch.pow(m_0_hat, torch.arange(1, 1 + 2 * (num_heads - n), 2))
          m = torch.cat([m, m_hat])

        return m.view(1, num_heads, 1, 1)


    def _create_causal_mask(self, max_seq_len: int) -> Tensor:
        """Create causal mask with ones where tokens can attend to each other.

        Returns:
            Tensor with causal mask, shape: [1, 1, seq len, seq len]
        """
        t = torch.ones((max_seq_len, max_seq_len))
        t = torch.tril(t, diagonal = 0)
        reshaped_t = torch.reshape(t, (1, 1, max_seq_len, max_seq_len))
        res_tb = reshaped_t.to(dtype=torch.bool)

        return res_tb

    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:
        """Apply Self-Attention to input data with respect to pad tokens.

        Args:
            x: input tensor, shape [bs, seq len, hidden dim]
            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]
        Returns:
            result tensor, shape [bs, seq len, hidden dim]
        """
        q = self.q_proj(x)
        kv = self.kv_proj(x)
        k, v = torch.split(kv, self.config.n_kv_head * self.head_dim, dim=2)

        bs = x.size(0)
        seq_len = x.size(1)

        q = q.view([bs, seq_len, self.config.n_head, self.head_dim])
        q = torch.transpose(q, 1, 2)
        k = k.view([bs, seq_len, self.config.n_kv_head, self.head_dim])
        k = torch.transpose(k, 1, 2)
        v = v.view([bs, seq_len, self.config.n_kv_head, self.head_dim])
        v = torch.transpose(v, 1, 2)

        if self.config.n_kv_head != self.config.n_head:
            k = k.repeat_interleave(self.q_per_kv, dim=1)
            v = v.repeat_interleave(self.q_per_kv, dim=1)

        att_scores = (q @ k.transpose(-2, -1)) * self.scale

        positions = torch.arange(seq_len, device=x.device)
        rel_pos = positions.view(1, 1, seq_len, 1) - positions.view(1, 1, 1, seq_len)
        rel_pos = rel_pos.clamp(min=0)
        alibi_bias = self.alibi * rel_pos
        att_scores = att_scores - alibi_bias

        causal = self.causal_mask[:, :, :seq_len, :seq_len]
        att_scores = att_scores.masked_fill(~causal, float("-inf"))

        if attention_mask is not None:
            if attention_mask.dim() == 3:
                attention_mask = attention_mask.squeeze(-1)
            key_mask = attention_mask[:, None, None, :].bool()
            att_scores = att_scores.masked_fill(~key_mask, float("-inf"))

        att_weights = F.softmax(att_scores, dim=-1)
        att_weights = self.attn_dropout(att_weights)

        out = torch.matmul(att_weights, v)
        out = out.transpose(1, 2).contiguous().view(bs, seq_len, self.config.hidden_dim)
        out = self.out_proj(out)
        return out

class SwiGLU(nn.Module):
    def __init__(self, config: TransformerConfig):
        """Gated Liner Unit with Swish Activation"""
        super().__init__()
        self.config = config
        # Init up- and down- projection layers
        self.fc1 = nn.Linear(self.config.hidden_dim, 2 * self.config.intermediate_dim, bias=False)
        self.fc2 = nn.Linear(self.config.intermediate_dim, self.config.hidden_dim, bias = False)

    def forward(self, x: Tensor) -> Tensor:
        """Apply SwiGLU to input data.

        Args:
            x: input tensor, shape [bs, seq len, hidden dim]
        Returns:
            result tensor, shape [bs, seq len, hidden dim]
        """
        x1 = self.fc1(x)
        gate, value = torch.split(x1, self.config.intermediate_dim, dim=-1)

        y = F.silu(gate) * value
        data = self.fc2(y)
        return data

class Block(nn.Module):
    def __init__(self, config: TransformerConfig):
        """Base Transformer Block
        - Causal Self-Attention and SwiGLU as main elements
        - Pre-normalization via RMSNorm
        - Regularization with dropouts before residuals
        """
        super().__init__()
        self.ln_1 = RMSNorm(config.hidden_dim)
        self.res_dropout_1 = nn.Dropout(config.dropout)
        self.attn = CausalSelfAttention(config)

        self.ln_2 = RMSNorm(config.hidden_dim)
        self.res_dropout_2 = nn.Dropout(config.dropout)
        self.mlp = SwiGLU(config)

    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:
        """Apply Transformer Block to input data.

        Args:
            x: input tensor, shape [bs, seq len, hidden dim]
            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]
        Returns:
            result tensor, shape [bs, seq len, hidden dim]
        """
        x_norm = self.ln_1(x)
        self_att = self.attn(x_norm, attention_mask)
        x = x + self.res_dropout_1(self_att)

        x_norm2 = self.ln_2(x)
        swig = self.mlp(x_norm2)
        x = x + self.res_dropout_2(swig)

        return x

class TransformerForCausalLM(nn.Module, PyTorchModelHubMixin):
    def __init__(self, config: TransformerConfig):
        """Transformer model for Language Modeling"""
        super().__init__()
        self.vocab_size = config.vocab_size
        self.max_seq_len = config.max_seq_len
        self.n_layer = config.n_layer
        self.n_head = config.n_head
        self.hidden_dim = config.hidden_dim
        self.dropout = config.dropout

        self.token_emb = nn.Embedding(self.vocab_size, self.hidden_dim)
        self.emb_dropout = nn.Dropout(config.dropout)
        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])
        self.ln_final = RMSNorm(config.hidden_dim)
        self.lm_head = nn.Linear(self.hidden_dim, self.vocab_size, bias = False)

        self.apply(self._init_weights)

        n_params = sum(p.numel() for p in self.parameters())
        print(f"Number of parameters: {n_params / 1e6:.2f}M")

    def _save_pretrained(self, save_directory: str):
      os.makedirs(save_directory, exist_ok=True)
      model_path = os.path.join(save_directory, "pytorch_model.bin")
      torch.save(self.state_dict(), model_path)

    @classmethod
    def _from_pretrained(cls, resolved_archive_file, *model_args, config=None, **kwargs):
        model = cls(config, *model_args, **kwargs)
        state_dict = torch.load(resolved_archive_file, map_location="cpu")
        model.load_state_dict(state_dict)
        return model

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, RMSNorm):
            torch.nn.init.ones_(module.scale)

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> Tensor:
        """Calculate logits for given input ids.

        Args:
            x: input tensor, shape [bs, seq len, hidden dim]
            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]
        Returns:
            logits, shape [bs, seq len, hidden dim]
        """
        x = self.token_emb(input_ids)
        x = self.emb_dropout(x)
        for block in self.layers:
            x = block(x, attention_mask=attention_mask)
        x = self.ln_final(x)
        logits = self.lm_head(x)

        return logits

    @torch.inference_mode()
    def generate(
        self, idx: Tensor, max_new_tokens, eos_token_id, temperature=1.0, do_sample=False, top_k=None
    ) -> Tensor:
        """Take a conditioning sequence of indices and complete the sequence max_new_tokens times,
        feeding the predictions back into the model each time.

        Args:
            idx: tensor with conditional tokens, shape [seq len]
            max_new_tokens: maximum number of new tokens
            eos_token_id: index of EOS token to stop generation
            temperature, do_sample, top_k: generation parameters
        Return:
            tensor with generated indexes
        """
        for _ in range(max_new_tokens):
            idx_cond = idx if idx.shape[1] <= self.max_seq_len else idx[:, -self.max_seq_len :]
            logits = self(idx_cond)

            logits = logits[:, -1, :] / max(temperature, 1e-10)

            if top_k is not None:
                vals, _ = torch.topk(logits, top_k, dim = -1)
                thresh = vals[..., -1, None]
                mask = logits < thresh
                logits[mask] = -float("inf")

            probs = F.softmax(logits, dim = -1)

            if do_sample:
                idx_next = torch.multinomial(probs, num_samples = 1)
            else:
                _, idx_next = torch.max(probs, dim = -1, keepdim = True)

            idx = torch.cat((idx, idx_next), dim=1)
            if (idx_next == eos_token_id).all():
                break
        return idx

"""# Train Loop"""

# –û–ø—Ä–µ–¥–µ–ª–∏–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–∞–∫ –∑–∞–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å —Å–µ–º–ø–ª—ã –≤ –±–∞—Ç—á
# –†–∞–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –¥–ª–∏–Ω—É, –ø–æ—ç—Ç–æ–º—É –±—É–¥–µ—Ç –ø–∞–¥–∏—Ç—å –¥–æ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–µ–º–ø–ª–∞
# –¢–∞–∫ –∂–µ –∑–∞–≤–µ–¥–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –º–∞—Å–∫—É, —á—Ç–æ–±—ã –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª –ø–∞–¥–∏–Ω–≥–∏


class TextDataset(torch.utils.data.Dataset):
    def __init__(self, texts, tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        texts = self.texts[idx]
        tokenized_sequence = self.tokenizer.encode(texts)
        return tokenized_sequence


def data_collator(
    tokenized_sequences: list[list[int]], pad_token_id: int, max_seq_len: int = None
) -> tuple[torch.Tensor, torch.Tensor]:
    batch_size = len(tokenized_sequences)
    max_batch_seq_len = min(max_seq_len, max((len(it) for it in tokenized_sequences)))

    input_ids = torch.full((batch_size, max_batch_seq_len), pad_token_id)
    attention_mask = torch.zeros((batch_size, max_batch_seq_len))

    for i, tok_seq in enumerate(tokenized_sequences):
        cur_len = min(len(tok_seq), max_batch_seq_len)
        input_ids[i, :cur_len] = torch.tensor(tok_seq[:cur_len])
        attention_mask[i, :cur_len] = 1

    return input_ids, attention_mask


def create_dataloader(dataset, pad_token_id, max_seq_len, batch_size, is_train):
    collate_fn = partial(data_collator, pad_token_id=pad_token_id, max_seq_len=max_seq_len)
    return DataLoader(
        dataset, batch_size=batch_size, shuffle=is_train, drop_last=is_train, collate_fn=collate_fn, pin_memory=True
    )


_d = TextDataset(["–ü—Ä–∏–≤–µ—Ç!", "–ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?", "–û—Å—Ç–∞–ª–æ—Å—å —Å–æ–≤—Å–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–æ –∫–æ–Ω—Ü–∞"], tokenizer)
_dl = create_dataloader(_d, tokenizer.eos_token_id, max_seq_len=16, batch_size=2, is_train=False)

for i, batch in enumerate(_dl):
    print(f"Batch #{i}")
    input_ids, attn_mask = batch
    print(input_ids, attn_mask, sep="\n\n")

def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """Scheduler for Optimizer with linear warmup and linear decay to the end of training

    Args:
        optimizer: torch optimizer to control learning rate
        num_warmup_steps: number of warmup steps
        num_training_steps: total number of training steps
    Return:
        torch learning rate scheduler
    """
    assert num_training_steps >= num_warmup_steps

    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
          return float(current_step) / float(max(1, num_warmup_steps))
        return max(0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def cross_entropy_loss(input_ids: Tensor, attention_mask: Tensor, logits: Tensor) -> Tensor:
    """Calculate Cross-Entropy loss for Language Modeling task
    Under the hood:
    1. Create targets based on input ids
    2. Masked out tokens corresponded to paddings
    3. Calculate cross entropy loss

    Args:
        input_ids: tensor with input ids, shape [bs, seq len]
        attention_mask: mask with zeros for pad tokens, shape [bs, seq len]
        logits: predicted logits, shape [bs, seq len, vocab size]
    Return:
        cross entropy loss, single-item tensor
    """
    sh_logs = logits[:, :-1, :].contiguous()
    sh_label = input_ids[:, 1:].contiguous()
    sh_mask = attention_mask[:, 1:].contiguous()

    voc_size = sh_logs.size(-1)
    loss = F.cross_entropy(sh_logs.view(-1, voc_size),
                           sh_label.view(-1),
                           reduction = 'none')
    loss = loss * sh_mask.view(-1)

    den = sh_mask.sum().clamp_min(1.0)
    loss = loss.sum() / den
    return loss

# –û–ø—Ä–µ–¥–µ–ª–∏–º —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

class Trainer:

    def __init__(
        self,
        learning_rate=3e-4,
        weight_decay=0.01,
        clip_grad_norm=1.0,
        n_steps=10_000,
        val_every_n_steps=1_000,
        plot_every_n_steps=100,
    ):
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.clip_grad_norm = clip_grad_norm
        self.n_steps = n_steps
        self.val_every_n_steps = val_every_n_steps
        self.plot_every_n_steps = plot_every_n_steps

        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"
        print("running on device", self.device)

    @torch.no_grad()
    def validate(self, model, val_loader):
        model.eval()
        val_loss = 0.0
        for batch in tqdm(val_loader, desc="Validating", leave=False):
            input_ids, attention_mask = batch
            input_ids = input_ids.to(self.device, non_blocking=True)
            attention_mask = attention_mask.to(self.device, non_blocking=True)

            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]
            val_loss += cross_entropy_loss(input_ids, attention_mask, logits)
        return val_loss / len(val_loader)

    def run(self, model, train_loader, val_loader):
        model = model.to(self.device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps=0.1 * self.n_steps, num_training_steps=self.n_steps
        )
        model.train()

        plotlosses = PlotLosses(figsize=(15, 9), step_names="Step")
        logs = {"lr": 0, "epoch": 0}

        data_iter = iter(train_loader)
        for iter_num in range(self.n_steps):
            try:
                batch = next(data_iter)
            except StopIteration:
                data_iter = iter(train_loader)
                logs["epoch"] += 1
                batch = next(data_iter)

            input_ids, attention_mask = batch
            input_ids = input_ids.to(self.device, non_blocking=True)
            attention_mask = attention_mask.to(self.device, non_blocking=True)

            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]
            loss = cross_entropy_loss(input_ids, attention_mask, logits)

            # backprop and update the parameters
            model.zero_grad(set_to_none=True)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad_norm)
            optimizer.step()
            scheduler.step()

            if iter_num > 0 and iter_num % self.val_every_n_steps == 0:
                val_loss = self.validate(model, val_loader)
                plotlosses.update({"val_loss": val_loss.item()}, current_step=iter_num)
                plotlosses.send()
                model.train()

            if iter_num % self.plot_every_n_steps == 0:
                logs["loss"] = loss.item()
                logs["lr"] = scheduler.get_last_lr()[0]
                plotlosses.update(logs, current_step=iter_num)
                plotlosses.send()

        val_loss = self.validate(model, val_loader)
        plotlosses.update({"val_loss": val_loss.item()}, current_step=iter_num)
        plotlosses.send()

# –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã

MAX_SEQ_LEN = 128
BATCH_SIZE = 16

train_dataset = TextDataset(dataset["train"]["jokes"], tokenizer)
train_dataloader = create_dataloader(
    train_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=True
)

test_dataset = TextDataset(dataset["test"]["jokes"], tokenizer)
test_dataloader = create_dataloader(
    test_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=False
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å

config = model_configs["nano"]
model = TransformerForCausalLM(config)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞

trainer = Trainer(learning_rate=3e-4)

# –û–±—É—á–µ–Ω–∏–µ goes brrrr!

trainer.run(model, train_dataloader, test_dataloader)

# –°–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–ª–∞–∑–∞–º–∏
# –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∏ —Å–ª–∞–±—ã—Ö –º–æ–¥–µ–ª–µ–π "–∑–∞—Ç—è–≥–∏–≤–∞–µ–º" –≥–∞–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

text = "–ó–∞—Ö–æ–¥–∏—Ç –≤ –±–∞—Ä"
input_ids = torch.tensor(tokenizer.encode(text)[:-1], device=trainer.device)[None, :]
print(input_ids)
model_output = model.generate(
    input_ids, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10
)
tokenizer.decode(model_output[0].tolist())

for prompt in [
    "–í—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫",
    "–°—Ç—É–¥–µ–Ω—Ç –ú–§–¢–ò –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ",
    "–°–∫–æ–ª—å–∫–æ —Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫–æ–≤ –Ω—É–∂–Ω–æ",
    "–®—Ç–∏—Ä–ª–∏—Ü —É—Å–ª—ã—à–∞–ª"
]:
    input_ids = torch.tensor(tokenizer.encode(prompt)[:-1], device=trainer.device)[None, :]
    output = model.generate(
        input_ids, max_new_tokens=1000, eos_token_id=tokenizer.eos_token_id,
        do_sample=True, top_k=20, temperature=0.7   #0.7
    )
    print(f"\nPROMPT: {prompt}\n---\n{tokenizer.decode(output[0].tolist())}\n")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ö–∞–±

model.push_to_hub(REPO_NAME)

